{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPiC Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#===========================================================================\n",
      "INFO: created directory: ../../results/qcd_to_top.CFM.EPiC.2024.02.22_11h17\n",
      "#===========================================================================\n",
      "+----------------------+-----------------+\n",
      "| Parameters           | Values          |\n",
      "+----------------------+-----------------+\n",
      "| NAME                 | qcd_to_top      |\n",
      "| DATASET              | jetclass        |\n",
      "| DATA_SOURCE          | qcd             |\n",
      "| DATA_TARGET          | top             |\n",
      "| MAX_NUM_CONSTITUENTS | 30              |\n",
      "| FEATURES             | constituents    |\n",
      "| PREPROCESS           | ['standardize'] |\n",
      "| DIM_INPUT            | 3               |\n",
      "| DEVICE               | cuda:3          |\n",
      "| OPTIMIZER            | Adam            |\n",
      "| LR                   | 0.0001          |\n",
      "| WEIGHT_DECAY         | 0.0             |\n",
      "| OPTIMIZER_BETAS      | [0.9, 0.999]    |\n",
      "| OPTIMIZER_EPS        | 1e-08           |\n",
      "| OPTIMIZER_AMSGRAD    | False           |\n",
      "| GRADIENT_CLIP        | 1.0             |\n",
      "| SCHEDULER            |                 |\n",
      "| SCHEDULER_T_MAX      |                 |\n",
      "| SCHEDULER_ETA_MIN    |                 |\n",
      "| SCHEDULER_GAMMA      |                 |\n",
      "| SCHEDULER_STEP_SIZE  |                 |\n",
      "| EPOCHS               | 3               |\n",
      "| BATCH_SIZE           | 1024            |\n",
      "| DATA_SPLIT_FRACS     | [0.8, 0.2, 0.0] |\n",
      "| NUM_WORKERS          | 0               |\n",
      "| PIN_MEMORY           | False           |\n",
      "| EARLY_STOPPING       |                 |\n",
      "| MIN_EPOCHS           |                 |\n",
      "| PRINT_EPOCHS         | 10              |\n",
      "| FIX_SEED             |                 |\n",
      "| MODEL                | EPiC            |\n",
      "| POOLING              | mean_sum        |\n",
      "| DIM_HIDDEN           | 300             |\n",
      "| TIME_EMBEDDING       | sinusoidal      |\n",
      "| DIM_TIME_EMB         | 12              |\n",
      "| DIM_GLOBAL           | 10              |\n",
      "| NUM_EPIC_LAYERS      | 6               |\n",
      "| ACTIVATION           | ReLU            |\n",
      "| DYNAMICS             | CFM             |\n",
      "| SIGMA                | 1e-05           |\n",
      "| AUGMENTED            | False           |\n",
      "| T0                   | 0.0             |\n",
      "| T1                   | 1.0             |\n",
      "| SAMPLER              | NeuralODE       |\n",
      "| SOLVER               | midpoint        |\n",
      "| NUM_SAMPLING_STEPS   | 200             |\n",
      "| SENSITIVITY          | adjoint         |\n",
      "| ATOL                 |                 |\n",
      "| RTOL                 |                 |\n",
      "| NUM_GEN_SAMPLES      | 10000           |\n",
      "+----------------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 11:17:33.138540: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-22 11:17:33.183002: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-22 11:17:33.184751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-22 11:17:33.906564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "number of training parameters: 2526746\n",
      "start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 0.8/0.2/0.0\n",
      "INFO: train size: 80000, validation size: 20000, testing sizes: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb278281a464545ad4f59e4eae57c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n",
      "> torch.Size([1024, 1, 1]) torch.Size([1024, 30, 3])\n",
      ">> torch.Size([1024, 1, 12]) torch.Size([1024, 30, 3])\n",
      ">>> torch.Size([1024, 30, 12])\n",
      ">>> torch.Size([1024, 30, 15])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDynGenModels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperiment\u001b[39;00m \u001b[39mimport\u001b[39;00m Experiment\n\u001b[1;32m      4\u001b[0m cfm \u001b[39m=\u001b[39m Experiment(Config_JetClass_EPiC_CondFlowMatch,\n\u001b[1;32m      5\u001b[0m                  NAME \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mqcd_to_top\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                  DATA_SOURCE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mqcd\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                  NUM_SAMPLING_STEPS \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m,\n\u001b[1;32m     25\u001b[0m                  DEVICE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda:3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m cfm\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/DynGenModels/src/DynGenModels/models/experiment.py:200\u001b[0m, in \u001b[0;36mExperiment.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/DynGenModels/src/DynGenModels/models/experiment.py:65\u001b[0m, in \u001b[0;36mDefineModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m#...train\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     train\u001b[39m.\u001b[39;49mupdate(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, loss_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamics\u001b[39m.\u001b[39;49mloss, dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader\u001b[39m.\u001b[39;49mtrain, optimizer\u001b[39m=\u001b[39;49moptimizer) \n\u001b[1;32m     66\u001b[0m     valid\u001b[39m.\u001b[39mupdate(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, loss_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamics\u001b[39m.\u001b[39mloss, dataloader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader\u001b[39m.\u001b[39mvalid, seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfix_seed)\n\u001b[1;32m     67\u001b[0m     TERMINATE, IMPROVED \u001b[39m=\u001b[39m valid\u001b[39m.\u001b[39mcheckpoint(min_epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_epochs, early_stopping\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearly_stopping)\n",
      "File \u001b[0;32m~/DynGenModels/src/DynGenModels/models/utils.py:23\u001b[0m, in \u001b[0;36mTrain_Step.update\u001b[0;34m(self, model, loss_fn, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m     loss_current \u001b[39m=\u001b[39m loss_fn(model, batch)\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/flow_match_env/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/DynGenModels/src/DynGenModels/datamodules/jetclass/datasets.py:24\u001b[0m, in \u001b[0;36mJetClassDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[39m=\u001b[39m {}\n\u001b[1;32m     23\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_preprocess[idx]\n\u001b[0;32m---> 24\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_preprocess[idx]\n\u001b[1;32m     25\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39mtarget context\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjets_target[idx]\n\u001b[1;32m     26\u001b[0m output[\u001b[39m'\u001b[39m\u001b[39msource context\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjets_source[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from DynGenModels.configs.registered_experiments import Config_JetClass_EPiC_CondFlowMatch\n",
    "from DynGenModels.models.experiment import Experiment\n",
    "\n",
    "cfm = Experiment(Config_JetClass_EPiC_CondFlowMatch,\n",
    "                 NAME = 'qcd_to_top',\n",
    "                 DATA_SOURCE = 'qcd',\n",
    "                 DATA_TARGET = 'top',\n",
    "                 MAX_NUM_CONSTITUENTS = 30,\n",
    "                 DYNAMICS = 'CFM',\n",
    "                 DATA_SPLIT_FRACS = [0.8, 0.2, 0.0],\n",
    "                 PREPROCESS = ['standardize'],\n",
    "                 BATCH_SIZE = 1024,\n",
    "                 EPOCHS = 3,\n",
    "                 PRINT_EPOCHS = 10,\n",
    "                 LR = 1e-4,\n",
    "                 GRADIENT_CLIP = 1.0,\n",
    "                 DIM_HIDDEN = 300,\n",
    "                 TIME_EMBEDDING = 'sinusoidal',\n",
    "                 USE_SKIP_CONNECTIONS = True,\n",
    "                 DIM_TIME_EMB = 12,\n",
    "                 DIM_GLOBAL = 10,\n",
    "                 NUM_EPIC_LAYERS = 6,\n",
    "                 SIGMA = 1e-5,\n",
    "                 SOLVER ='midpoint',\n",
    "                 NUM_SAMPLING_STEPS = 200,\n",
    "                 DEVICE = 'cuda:3')\n",
    "\n",
    "cfm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_features\n",
    "\n",
    "plot_features(cfm, features=[r'$p_T$', r'$\\eta$', r'$\\phi$', r'$m$'], figsize=(14, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_match_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
