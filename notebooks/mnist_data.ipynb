{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from DynGenModels.trainer.configs import Training_Configs\n",
    "\n",
    "@dataclass\n",
    "class Configs(Training_Configs):\n",
    "    DATA_SOURCE = None\n",
    "    DATA_TARGET = 'mnist'\n",
    "    dim_input = (1, 28, 28)\n",
    "    batch_size = 10\n",
    "    MODEL : str = 'Unet'\n",
    "    dim_hidden : int = 32\n",
    "    num_res_blocks : int = 1\n",
    "    DYNAMICS : str = 'CondFlowMatch'\n",
    "    sigma : float = 0.001\n",
    "    augmented : bool = False\n",
    "    t0 : float = 0.0\n",
    "    t1 : float = 1.0\n",
    "\n",
    "class MNIST_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, configs: dataclass):\n",
    "        self.configs = configs\n",
    "        self.get_target_data()\n",
    "        self.get_source_data()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        output = {}\n",
    "        output['target'] = self.target[idx]\n",
    "        output['source'] = self.source[idx]\n",
    "        output['context'] = self.target_label[idx]\n",
    "        output['mask'] = torch.ones_like(output['target'])\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "    def get_target_data(self):\n",
    "        \n",
    "        if self.configs.DATA_TARGET == 'mnist':\n",
    "            self.data_1 = datasets.MNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        \n",
    "        elif self.configs.DATA_TARGET == 'emnist':\n",
    "            self.data_1 = datasets.EMNIST(root='../data', split='letters', train=True, download=True, transform=transforms.ToTensor())\n",
    "        \n",
    "        elif self.configs.DATA_TARGET == 'fashion':\n",
    "             self.data_1 = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        \n",
    "        self.target = [d[0] for d in self.data_1]\n",
    "        self.target_label = [d[1] for d in self.data_1]\n",
    "        \n",
    "    def get_source_data(self):\n",
    "\n",
    "        if self.configs.DATA_SOURCE == 'mnist':\n",
    "            self.data_0 = datasets.MNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "       \n",
    "        elif self.configs.DATA_SOURCE == 'emnist':\n",
    "            self.data_0 = datasets.EMNIST(root='./data', split='letters', train=True, download=True, transform=transforms.ToTensor())\n",
    "       \n",
    "        elif self.configs.DATA_SOURCE == 'fashion':\n",
    "             self.data_0 = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "        else:\n",
    "            self.data_0 = self.data_1\n",
    "\n",
    "        self.source = [d[0] for d in self.data_0] if self.configs.DATA_SOURCE is not None else [torch.rand_like(d[0]) for d in self.data_0]\n",
    "        self.source_label = [d[1] for d in self.data_0] if self.configs.DATA_SOURCE is not None else [0] * len(self.data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class MNISTDataLoader:\n",
    "\n",
    "    def __init__(self, datasets: Dataset, configs: dataclass): # type: ignore\n",
    "\n",
    "        self.datasets = datasets        \n",
    "        self.fracs = configs.data_split_fracs\n",
    "        self.batch_size = configs.batch_size\n",
    "        self.dataloader()\n",
    "\n",
    "    def train_val_test_split(self, shuffle=False):\n",
    "        assert np.abs(1.0 - sum(self.fracs)) < 1e-3, \"Split fractions do not sum to 1!\"\n",
    "        total_size = len(self.datasets)\n",
    "        train_size = int(total_size * self.fracs[0])\n",
    "        valid_size = int(total_size * self.fracs[1])\n",
    "\n",
    "        #...define splitting indices\n",
    "\n",
    "        idx = torch.randperm(total_size) if shuffle else torch.arange(total_size)\n",
    "        idx_train = idx[:train_size].tolist()\n",
    "        idx_valid = idx[train_size : train_size + valid_size].tolist()\n",
    "        idx_test = idx[train_size + valid_size :].tolist()\n",
    "        \n",
    "        #...Create Subset for each split\n",
    "\n",
    "        train_set = Subset(self.datasets, idx_train)\n",
    "        valid_set = Subset(self.datasets, idx_valid) if valid_size > 0 else None\n",
    "        test_set = Subset(self.datasets, idx_test) if self.fracs[2] > 0 else None\n",
    "\n",
    "        return train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        print(\"INFO: building dataloaders...\")\n",
    "        print(\"INFO: train/val/test split ratios: {}/{}/{}\".format(self.fracs[0], self.fracs[1], self.fracs[2]))\n",
    "        \n",
    "        train, valid, test = self.train_val_test_split(shuffle=True)\n",
    "        self.train = DataLoader(dataset=train, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid = DataLoader(dataset=valid,  batch_size=self.batch_size, shuffle=False) if valid is not None else None\n",
    "        self.test = DataLoader(dataset=test,  batch_size=self.batch_size, shuffle=True) if test is not None else None\n",
    "\n",
    "        print('INFO: train size: {}, validation size: {}, testing sizes: {}'.format(len(self.train.dataset),  # type: ignore\n",
    "                                                                                    len(self.valid.dataset if valid is not None else []),  # type: ignore\n",
    "                                                                                    len(self.test.dataset if test is not None else []))) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Configs()\n",
    "config.workdir = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 1.0/0.0/0.0\n",
      "INFO: train size: 60000, validation size: 0, testing sizes: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAAES0lEQVR4nH2SeTDcZxjH392sYBO7uo6kyTKDHFNXkCx1ZuNaDHEuGjqtSnVkXI2jpI6myCS6I844miC0ziCYUCpRVty2WBokP2dk7dpDdrds2B/9Q8NiZr//PTOfzzzv+31f4OFf7HH+d8d2yutpu+cadDo2+brIRfD3YNn6nTAgJUjNrbWtKW2bJQdX7RcirP8X6cXz7w3jFZfXsDOrUkUjzdFNewwNKbbguEPY8PXYEyO+5im6uo1+16KkiQj6p/MmZxM/005JKVglPx66yPuck/HOjfiQmQ0pXJe2sY157prtajgdbRY+VNOqQr7B6zpOm8Gfq4VwHbuU4YpIRIk5vc/MnesIME9mqfz1WL+3gii77LRgF5ysICL0sS0xu1AwDMMwPKYmedT+K28QLTW4W6HWs9B3J1tHhvPOpPYdw12geNWhtz9SVklLAuCpMu6ysGd2bwjE9bOwjKMrgcStmBt7YOx0NX/se9nM9jWHPQqNAiAfhkkSK5mNkT9gZh/qyNOP/vtUNmyC9I0M09himg1PyaL31yE/CMdIjL8p371S4CSPs+xJxzaEUKHSOxFadmwfndsc36IDRZLhfolpFMqcOAv9U8cw+SMg9/nKZZoVg8rFD4gzyA8qDojWsJC4OyDvkVyw/oRXibC1kTDS+Xgd78n4i2aHkLKTmPQVcDDye0WjQiGyTeLPKU6svslOftqjd13LFXznLMH0W6jE8ZAoEdR9DZ9KK2MbvROllyyHc1wZpwxaS6pdDbzQZHfDrv2sOhBy98SFPxOKFiKQUV4IpQybtK94ZHWTq50rSWuMuOr+/Z52BFj8oMdd2pkQldXFuE1TVdYHTXNmEDUm9WUej6AVwozz0w4lte9KOLyutxUWACBkTIKWHjoAAONWWvvWaTQo80k0X3Ep72nb5KRBP3PDzrFX5PS/db6sagCWzGYVAKia2olhubBTxK+LmgXfrrAWQ9H5QXbkvCT8DczijhdQiBSwxSgAGCKA2AYIsI24BAAgv9Gv3MCQPCOqEniN5BJn/fux9nF5PiMj8uXxO+I2zE89Ng/DP6lK3hiZVTq1+uPt5vFtWNXT7wihJoSWLLR1Bi/rzC7QPkIztCb8Rncua19Xp5FB2Z1zfp1uvcjE/vJf6JHsLerl7maumPsqZ4cQwTAMi748+JCWMW1tsZzWeU6WGv2ibjR6TutoeNhNpJqgsGNwhzBtWF9f9zv0A8YtlEHcSEOB+b3yhWcGtA04kBDbgp4V3izMf30IlggqCmJ02g8oUSHSreXAJZoH0UW5SwYyN8MJKWxpItBL4+dQ5qo5UbwrZurd3k1yeahoVASxzucRp0nqRj2jwF9tq4ep0fPefc/Q7fwj7gy5HjHKWQM/wJEmIhMo6qY6Q2tio+wOxf4GpaHgclQNa4MZyrb9ZEjqxoT2LAVfF4cznHiWibJB6sx2fbHaaOVEWn3Ie6408T9NEPX9MY18OQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=56x28>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.DATA_SOURCE = None\n",
    "config.DATA_TARGET = 'mnist'\n",
    "data = MNIST_Dataset(config)\n",
    "dataloader = MNISTDataLoader(data, config)\n",
    "\n",
    "transform = ToPILImage()\n",
    "\n",
    "for batch in dataloader.train:\n",
    "    pair = torch.cat([batch['source'][0].squeeze(), batch['target'][0].squeeze()], dim=1)\n",
    "    img = transform(pair)\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: building dataloaders...\n",
      "INFO: train/val/test split ratios: 1.0/0.0/0.0\n",
      "INFO: train size: 60000, validation size: 0, testing sizes: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADgAAAAcCAAAAAAR0CPiAAACaUlEQVR4nI3Uv2uTQRgH8O/z3OVHExprY9pUxf4wItil2sFBuoggdXAQVOzm4FqLg3+ACi52cZPODrq4iOImBbEIQhVBQW2hamxr09ikSfPjvXscSqy9e8Hc8ML7cB++zwN3BwAAWAEYBIAUABDaWwQAvdPyZuLK3fyP87ptAwxMTUT1b8mXs7o7UTIzt9qEtye5UjcKCS6JNSqW2sq2BYdfr0oE1toamRgTm2r/3Ln/QA0AN2rQSsciFsSE+la9Y/2sv5XvfZxxYC+hC9WFja2NurbJA+lOHXCy4sLLU9Nu4n7Ln16tN7nJqU0j1WxupAu5dy48hp8u3GPNXLEvSbVMo6OaaurPQjjkwU08+7dxANhopK4NxjOGTaFTfc1XqMei15uxvOtPA8ikm9H3fWtL9ehW8KFQi62M6tJSwoMnPHjcqOzjozTUKUrFS+XlX5WE4uv3HXfk4lrZKYGTiEi+uLay/H21UFwtPhh5/mjA3TRqZ91E2AqaqFYECBQZ6R6eH/caBfDUhcSqAbBAoDRzIyiDCNZhp7DiQjECwIJIYFnACxDx8i4EuxJ556vAzICQxEP6HBt7WfChILkoJBAQGyv9IfCw3IEPgbQCASKkIBILeQNOz8+GQEHNkCUA1moEGT8wd8kptBKJSAggEJGN+4m56IswSDgYNQSAQAIJeXTO0JPwxB4SagWbrp363yXObWnN2M0AMYiJYeIZr9WrD4PwViNksT2ZZizG4B6AL4FTaY3TEaAh3GQLW9NN79nASbewDS3G99p9wlAawLfE0FtvxnAouDmZZmOktlzaE5T7894R99Yf9+7qMFbQHYYAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=56x28>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Configs()\n",
    "config.DATA_SOURCE = 'fashion'\n",
    "config.DATA_TARGET = 'mnist'\n",
    "data = MNIST_Dataset(config)\n",
    "dataloader = MNISTDataLoader(data, config)\n",
    "\n",
    "transform = ToPILImage()\n",
    "\n",
    "for batch in dataloader.train:\n",
    "    pair = torch.cat([batch['source'][0].squeeze(), batch['target'][0].squeeze()], dim=1)\n",
    "    img = transform(pair)\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchcfm.models.unet import UNetModel\n",
    "\n",
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super().__init__()\n",
    "        self.device = configs.DEVICE\n",
    "        self.unet = UNetModel(dim=configs.dim_input, num_channels=configs.dim_hidden, num_res_blocks=configs.num_res_blocks)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, t, x, context=None, mask=None):\n",
    "        x = self.unet(t, x, y=context)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_channels must be divisible by num_groups",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Unet(config)\n",
      "Cell \u001b[0;32mIn[245], line 10\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m configs\u001b[39m.\u001b[39mDEVICE\n\u001b[0;32m---> 10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet \u001b[39m=\u001b[39m UNetModel(dim\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mdim_input, num_channels\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mdim_hidden, num_res_blocks\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mnum_res_blocks)\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:904\u001b[0m, in \u001b[0;36mUNetModelWrapper.__init__\u001b[0;34m(self, dim, num_channels, num_res_blocks, channel_mult, learn_sigma, class_cond, num_classes, use_checkpoint, attention_resolutions, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, dropout, resblock_updown, use_fp16, use_new_attention_order)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m attention_resolutions\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    902\u001b[0m     attention_ds\u001b[39m.\u001b[39mappend(image_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mint\u001b[39m(res))\n\u001b[0;32m--> 904\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    905\u001b[0m     image_size\u001b[39m=\u001b[39;49mimage_size,\n\u001b[1;32m    906\u001b[0m     in_channels\u001b[39m=\u001b[39;49mdim[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    907\u001b[0m     model_channels\u001b[39m=\u001b[39;49mnum_channels,\n\u001b[1;32m    908\u001b[0m     out_channels\u001b[39m=\u001b[39;49m(dim[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m learn_sigma \u001b[39melse\u001b[39;49;00m dim[\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m),\n\u001b[1;32m    909\u001b[0m     num_res_blocks\u001b[39m=\u001b[39;49mnum_res_blocks,\n\u001b[1;32m    910\u001b[0m     attention_resolutions\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(attention_ds),\n\u001b[1;32m    911\u001b[0m     dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m    912\u001b[0m     channel_mult\u001b[39m=\u001b[39;49mchannel_mult,\n\u001b[1;32m    913\u001b[0m     num_classes\u001b[39m=\u001b[39;49m(num_classes \u001b[39mif\u001b[39;49;00m class_cond \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    914\u001b[0m     use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint,\n\u001b[1;32m    915\u001b[0m     use_fp16\u001b[39m=\u001b[39;49muse_fp16,\n\u001b[1;32m    916\u001b[0m     num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[1;32m    917\u001b[0m     num_head_channels\u001b[39m=\u001b[39;49mnum_head_channels,\n\u001b[1;32m    918\u001b[0m     num_heads_upsample\u001b[39m=\u001b[39;49mnum_heads_upsample,\n\u001b[1;32m    919\u001b[0m     use_scale_shift_norm\u001b[39m=\u001b[39;49muse_scale_shift_norm,\n\u001b[1;32m    920\u001b[0m     resblock_updown\u001b[39m=\u001b[39;49mresblock_updown,\n\u001b[1;32m    921\u001b[0m     use_new_attention_order\u001b[39m=\u001b[39;49muse_new_attention_order,\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:461\u001b[0m, in \u001b[0;36mUNetModel.__init__\u001b[0;34m(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, dims, num_classes, use_checkpoint, use_fp16, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, resblock_updown, use_new_attention_order)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfor\u001b[39;00m level, mult \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(channel_mult):\n\u001b[1;32m    459\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_res_blocks):\n\u001b[1;32m    460\u001b[0m         layers \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 461\u001b[0m             ResBlock(\n\u001b[1;32m    462\u001b[0m                 ch,\n\u001b[1;32m    463\u001b[0m                 time_embed_dim,\n\u001b[1;32m    464\u001b[0m                 dropout,\n\u001b[1;32m    465\u001b[0m                 out_channels\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(mult \u001b[39m*\u001b[39;49m model_channels),\n\u001b[1;32m    466\u001b[0m                 dims\u001b[39m=\u001b[39;49mdims,\n\u001b[1;32m    467\u001b[0m                 use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint,\n\u001b[1;32m    468\u001b[0m                 use_scale_shift_norm\u001b[39m=\u001b[39;49muse_scale_shift_norm,\n\u001b[1;32m    469\u001b[0m             )\n\u001b[1;32m    470\u001b[0m         ]\n\u001b[1;32m    471\u001b[0m         ch \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(mult \u001b[39m*\u001b[39m model_channels)\n\u001b[1;32m    472\u001b[0m         \u001b[39mif\u001b[39;00m ds \u001b[39min\u001b[39;00m attention_resolutions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:167\u001b[0m, in \u001b[0;36mResBlock.__init__\u001b[0;34m(self, channels, emb_channels, dropout, out_channels, use_conv, use_scale_shift_norm, dims, use_checkpoint, up, down)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_checkpoint \u001b[39m=\u001b[39m use_checkpoint\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_scale_shift_norm \u001b[39m=\u001b[39m use_scale_shift_norm\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m--> 167\u001b[0m     normalization(channels),\n\u001b[1;32m    168\u001b[0m     nn\u001b[39m.\u001b[39mSiLU(),\n\u001b[1;32m    169\u001b[0m     conv_nd(dims, channels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdown \u001b[39m=\u001b[39m up \u001b[39mor\u001b[39;00m down\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m up:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/nn.py:84\u001b[0m, in \u001b[0;36mnormalization\u001b[0;34m(channels)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalization\u001b[39m(channels):\n\u001b[1;32m     79\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Make a standard normalization layer.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[39m    :param channels: number of input channels.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m    :return: an nn.Module for normalization.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m GroupNorm32(\u001b[39m32\u001b[39;49m, channels)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/normalization.py:252\u001b[0m, in \u001b[0;36mGroupNorm.__init__\u001b[0;34m(self, num_groups, num_channels, eps, affine, device, dtype)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39msuper\u001b[39m(GroupNorm, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m num_channels \u001b[39m%\u001b[39m num_groups \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mnum_channels must be divisible by num_groups\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_groups \u001b[39m=\u001b[39m num_groups\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels \u001b[39m=\u001b[39m num_channels\n",
      "\u001b[0;31mValueError\u001b[0m: num_channels must be divisible by num_groups"
     ]
    }
   ],
   "source": [
    "Unet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_channels must be divisible by num_groups",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[240], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDynGenModels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m DynGenModelTrainer\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDynGenModels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdynamics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcnf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcondflowmatch\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimalTransportFlowMatching \u001b[39mas\u001b[39;00m dynamics\n\u001b[1;32m      4\u001b[0m cfm \u001b[39m=\u001b[39m DynGenModelTrainer(dynamics \u001b[39m=\u001b[39m dynamics(config),\n\u001b[0;32m----> 5\u001b[0m                          model \u001b[39m=\u001b[39m Unet(config), \n\u001b[1;32m      6\u001b[0m                          dataloader \u001b[39m=\u001b[39m dataloader, \n\u001b[1;32m      7\u001b[0m                          configs \u001b[39m=\u001b[39m config)\n\u001b[1;32m      9\u001b[0m cfm\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[238], line 10\u001b[0m, in \u001b[0;36mUnet.__init__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m configs\u001b[39m.\u001b[39mDEVICE\n\u001b[0;32m---> 10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet \u001b[39m=\u001b[39m UNetModel(dim\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mdim_input, num_channels\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mdim_hidden, num_res_blocks\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mnum_res_blocks)\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:904\u001b[0m, in \u001b[0;36mUNetModelWrapper.__init__\u001b[0;34m(self, dim, num_channels, num_res_blocks, channel_mult, learn_sigma, class_cond, num_classes, use_checkpoint, attention_resolutions, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, dropout, resblock_updown, use_fp16, use_new_attention_order)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m attention_resolutions\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    902\u001b[0m     attention_ds\u001b[39m.\u001b[39mappend(image_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mint\u001b[39m(res))\n\u001b[0;32m--> 904\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    905\u001b[0m     image_size\u001b[39m=\u001b[39;49mimage_size,\n\u001b[1;32m    906\u001b[0m     in_channels\u001b[39m=\u001b[39;49mdim[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    907\u001b[0m     model_channels\u001b[39m=\u001b[39;49mnum_channels,\n\u001b[1;32m    908\u001b[0m     out_channels\u001b[39m=\u001b[39;49m(dim[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m learn_sigma \u001b[39melse\u001b[39;49;00m dim[\u001b[39m0\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m),\n\u001b[1;32m    909\u001b[0m     num_res_blocks\u001b[39m=\u001b[39;49mnum_res_blocks,\n\u001b[1;32m    910\u001b[0m     attention_resolutions\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(attention_ds),\n\u001b[1;32m    911\u001b[0m     dropout\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m    912\u001b[0m     channel_mult\u001b[39m=\u001b[39;49mchannel_mult,\n\u001b[1;32m    913\u001b[0m     num_classes\u001b[39m=\u001b[39;49m(num_classes \u001b[39mif\u001b[39;49;00m class_cond \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    914\u001b[0m     use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint,\n\u001b[1;32m    915\u001b[0m     use_fp16\u001b[39m=\u001b[39;49muse_fp16,\n\u001b[1;32m    916\u001b[0m     num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[1;32m    917\u001b[0m     num_head_channels\u001b[39m=\u001b[39;49mnum_head_channels,\n\u001b[1;32m    918\u001b[0m     num_heads_upsample\u001b[39m=\u001b[39;49mnum_heads_upsample,\n\u001b[1;32m    919\u001b[0m     use_scale_shift_norm\u001b[39m=\u001b[39;49muse_scale_shift_norm,\n\u001b[1;32m    920\u001b[0m     resblock_updown\u001b[39m=\u001b[39;49mresblock_updown,\n\u001b[1;32m    921\u001b[0m     use_new_attention_order\u001b[39m=\u001b[39;49muse_new_attention_order,\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:461\u001b[0m, in \u001b[0;36mUNetModel.__init__\u001b[0;34m(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, dims, num_classes, use_checkpoint, use_fp16, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, resblock_updown, use_new_attention_order)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfor\u001b[39;00m level, mult \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(channel_mult):\n\u001b[1;32m    459\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_res_blocks):\n\u001b[1;32m    460\u001b[0m         layers \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 461\u001b[0m             ResBlock(\n\u001b[1;32m    462\u001b[0m                 ch,\n\u001b[1;32m    463\u001b[0m                 time_embed_dim,\n\u001b[1;32m    464\u001b[0m                 dropout,\n\u001b[1;32m    465\u001b[0m                 out_channels\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(mult \u001b[39m*\u001b[39;49m model_channels),\n\u001b[1;32m    466\u001b[0m                 dims\u001b[39m=\u001b[39;49mdims,\n\u001b[1;32m    467\u001b[0m                 use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint,\n\u001b[1;32m    468\u001b[0m                 use_scale_shift_norm\u001b[39m=\u001b[39;49muse_scale_shift_norm,\n\u001b[1;32m    469\u001b[0m             )\n\u001b[1;32m    470\u001b[0m         ]\n\u001b[1;32m    471\u001b[0m         ch \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(mult \u001b[39m*\u001b[39m model_channels)\n\u001b[1;32m    472\u001b[0m         \u001b[39mif\u001b[39;00m ds \u001b[39min\u001b[39;00m attention_resolutions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/unet.py:167\u001b[0m, in \u001b[0;36mResBlock.__init__\u001b[0;34m(self, channels, emb_channels, dropout, out_channels, use_conv, use_scale_shift_norm, dims, use_checkpoint, up, down)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_checkpoint \u001b[39m=\u001b[39m use_checkpoint\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_scale_shift_norm \u001b[39m=\u001b[39m use_scale_shift_norm\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m--> 167\u001b[0m     normalization(channels),\n\u001b[1;32m    168\u001b[0m     nn\u001b[39m.\u001b[39mSiLU(),\n\u001b[1;32m    169\u001b[0m     conv_nd(dims, channels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels, \u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdown \u001b[39m=\u001b[39m up \u001b[39mor\u001b[39;00m down\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m up:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torchcfm/models/unet/nn.py:84\u001b[0m, in \u001b[0;36mnormalization\u001b[0;34m(channels)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalization\u001b[39m(channels):\n\u001b[1;32m     79\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Make a standard normalization layer.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[39m    :param channels: number of input channels.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m    :return: an nn.Module for normalization.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m GroupNorm32(\u001b[39m32\u001b[39;49m, channels)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/normalization.py:252\u001b[0m, in \u001b[0;36mGroupNorm.__init__\u001b[0;34m(self, num_groups, num_channels, eps, affine, device, dtype)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39msuper\u001b[39m(GroupNorm, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m num_channels \u001b[39m%\u001b[39m num_groups \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mnum_channels must be divisible by num_groups\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_groups \u001b[39m=\u001b[39m num_groups\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_channels \u001b[39m=\u001b[39m num_channels\n",
      "\u001b[0;31mValueError\u001b[0m: num_channels must be divisible by num_groups"
     ]
    }
   ],
   "source": [
    "from DynGenModels.trainer.trainer import DynGenModelTrainer\n",
    "from DynGenModels.dynamics.cnf.condflowmatch import OptimalTransportFlowMatching as dynamics\n",
    "\n",
    "cfm = DynGenModelTrainer(dynamics = dynamics(config),\n",
    "                         model = Unet(config), \n",
    "                         dataloader = dataloader, \n",
    "                         configs = config)\n",
    "\n",
    "cfm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
